{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdbd63a-9d50-493b-967f-0078c4e50214",
   "metadata": {},
   "source": [
    "run the mp scraper one and the actual hansard scraper one. then you'll have some html files of hansard and a csv file of mps. then run this nb.\n",
    "\n",
    "if u wanna find code that deals w edge cases, find comments that start with \"edge case\". examples are given. i started writing those comments q l8 tho so i don't guarantee that i commented on all of them. \n",
    "\n",
    "**the csv file you get from this is delimited by '|'. if reading with code make sure u account for that. if opening in excel, follow these instrns (https://support.affinity.co/hc/en-us/articles/360044453711-How-to-open-CSV-files-with-the-correct-delimiter-separator).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911324d7-4429-41bc-b979-769a705cd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import ast\n",
    "import os\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdb import set_trace as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9930a-84f1-483a-8415-3c2ec4d98245",
   "metadata": {},
   "source": [
    "**for merging with mps.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce675208-2543-4282-8a8f-eeb5d78f77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df = pd.read_csv('mps.csv')\n",
    "mp_df.Party = mp_df.Party.apply(ast.literal_eval)\n",
    "mp_df.Parliaments = mp_df.Parliaments.apply(ast.literal_eval)\n",
    "mps = dict(\n",
    "    zip(mp_df.Name.apply(lambda x: x.replace('.', '').replace(',', '').lower()), # keys\n",
    "    zip(mp_df.Name, mp_df.Party, mp_df.Parliaments))) # values\n",
    "mp_names = list(mps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c72765-1e10-4f66-b0c5-de5d4ad837c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alr_matched = set() # honorific+names that have alr been matched to names so we don't spam the print\n",
    "ministers_found = set() # minister titles that have alr been found (to be used for future searches in case of typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a41c038-c9f4-4b92-9313-fe5fccdab091",
   "metadata": {},
   "outputs": [],
   "source": [
    "honorific_regex = r'(mr|mrs|ms|miss|mdm|dr|er dr|prof|assoc prof|er|asst prof|assoc prof dr|inche|encik)'\n",
    "\n",
    "# for matching honorific+name in report to actual mp data.\n",
    "# cannot simply remove honorific as the programmer doesn't have an exhaustive list\n",
    "# of honorifics, and some are quite rare in everyday use (e.g. Inche Rahamat Bin Kenap).\n",
    "def honorific_name_to_mp_data(honorific_name):\n",
    "    honorific_name = honorific_name.replace('.','').replace(',','').replace(':','').lower().strip()\n",
    "    honorific_name = re.sub('\\(.+\\)', '', honorific_name)\n",
    "    \n",
    "    # try the easy way first (find and remove honorific)\n",
    "    honorific_match = re.match(honorific_regex, honorific_name)\n",
    "    if honorific_match:\n",
    "        name = honorific_name[honorific_match.span()[1]+1:]\n",
    "        if name in mps.keys():\n",
    "            return mps[name]\n",
    "        \n",
    "        # seems quite common for them to write \"asked\" twice in the hansard proceedings\n",
    "        last_asked = name.rfind(' asked')\n",
    "        if last_asked and name[:last_asked] in mps.keys():\n",
    "            return mps[name[:last_asked]]\n",
    "\n",
    "        # slightly harder way (rearranging words)\n",
    "        for mp_name in mp_names:\n",
    "            mp_name_words = set(mp_name.split(' '))\n",
    "            name_words = set(name.split(' '))\n",
    "            if mp_name_words == name_words:\n",
    "                if (honorific_name, mp_name) not in alr_matched:\n",
    "                    #print(f'rearranging matched {honorific_name} to {mp_name}')\n",
    "                    alr_matched.add((honorific_name, mp_name))\n",
    "                return mps[mp_name]\n",
    "            \n",
    "            # for omission of chinese name\n",
    "            if len(mp_name_words) - len(name_words) <= 2 and len(name_words) >= 2 and name_words.issubset(mp_name_words):\n",
    "                if (honorific_name, mp_name) not in alr_matched:\n",
    "                    print(f'allowing omitted words in name matched {honorific_name} to {mp_name}')\n",
    "                    alr_matched.add((honorific_name, mp_name))\n",
    "                return mps[mp_name]\n",
    "                \n",
    "    digit_match = re.search('\\d+', honorific_name)\n",
    "    if digit_match:\n",
    "        # names shldn't have digits\n",
    "        honorific_name = honorific_name[digit_match.span()[1]:]\n",
    "        return honorific_name_to_mp_data(honorific_name)\n",
    "        \n",
    "    # the hard way (levenshtein)\n",
    "    closest_name = levenshtein_best_match(honorific_name, mp_names)\n",
    "    \n",
    "    if (honorific_name, closest_name) not in alr_matched:\n",
    "        print(f'levenshtein matched {honorific_name} to {closest_name}')\n",
    "        alr_matched.add((honorific_name, closest_name))\n",
    "    return mps[closest_name]\n",
    "\n",
    "def levenshtein_best_match(value, options):\n",
    "    min_levenshtein = 99999\n",
    "    min_val = None\n",
    "    for option in options:\n",
    "        l_dist = levenshtein(option, value)\n",
    "        if l_dist < min_levenshtein:\n",
    "            min_levenshtein = l_dist\n",
    "            min_val = option\n",
    "    return min_val\n",
    "            \n",
    "\n",
    "# borrowed from: https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/\n",
    "# we use levenshtein as it helps to protect against typos too, like the \"asked asked\" in:\n",
    "# https://sprs.parl.gov.sg/search/sprs3topic?reportid=oral-answer-2822\n",
    "def levenshtein(token1, token2):\n",
    "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(token1)][len(token2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2eee96-c726-4f73-a0ab-a5b630687de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b55804-1d18-45fa-8518-78a1bacdc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportSection(Enum):\n",
    "    WRITTEN = 'Written Answers to Questions'\n",
    "    WRITTEN_NA = 'Written Answers to Questions for Oral Answer Not Answered by End of Question Time'\n",
    "    ORAL = 'Oral Answers to Questions'\n",
    "    BUDGET = 'Budget'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759f4b8-16fc-4450-a923-8bac17a53f4f",
   "metadata": {},
   "source": [
    "we assume that all pqs are prefaced with #. (non sprs) or # (sprs). ignore follow up qns since we are only interested in mapping mps to topics, and the follow up qns will always be from the same mp and on the same topic.\n",
    "\n",
    "notes regarding minister titles:\n",
    "* Minister for Culture, Community and Youth is the only minister title with a comma\n",
    "* but there used to be Minister for Information, Communication and the Arts and Minister for Community Development, Youth and Sports\n",
    "* no questions were ever directed to minister mentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd366a7a-0882-4584-a7ec-423a6a23fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_word = r'[A-Z][a-z]+'\n",
    "cap_words = f'({cap_word})( {cap_word})*'\n",
    "mccy = '(Acting )?Minister for Culture, Community and Youth'\n",
    "mica = '(Acting )?Minister for Information, Communications and the Arts'\n",
    "mcdys = '(Acting )?Minister for Community Development, Youth( and|,) Sports'\n",
    "micma = 'Minister-in-charge of Muslim Affairs'\n",
    "minister_for_something = f'({cap_words} )?Minister( of State)? (for|of) (the )?{cap_words}( and (the )?{cap_words})?'\n",
    "something_minister = f'{cap_words} Minister'\n",
    "one_minister_regex = f'(({mccy})|({mica})|({mcdys})|({micma})|({minister_for_something})|({something_minister}))'\n",
    "minister_regex = re.compile(f'{one_minister_regex}( and (the )?{one_minister_regex})?') # can have multiple targets\n",
    "\n",
    "def first_two_capitalized(words):\n",
    "    return words[0][0].isupper() and words[1][0].isupper() and words[0][1] and words[0][1].islower()\n",
    "\n",
    "def trim_off_non_pq_content_at_start(para):\n",
    "    para_words = para.split(' ')\n",
    "    if first_two_capitalized(para_words):\n",
    "        return para\n",
    "    \n",
    "    # i assume honorific+name has at least two words capitalized and non-numbers\n",
    "    while not first_two_capitalized(para_words):\n",
    "        para_words = para_words[1:]\n",
    "    \n",
    "    return ' '.join(para_words)\n",
    "\n",
    "# extracts the first substring which is a substring of ministers\n",
    "def extract_first_ministers(para):\n",
    "    minister_match = re.search(minister_regex, para)\n",
    "    if not minister_match:\n",
    "        # report might've been in the wrong case; try to match to existing ministers\n",
    "        minister_match = re.search(\n",
    "            '(' + '|'.join(list(map(lambda s: s.lower().replace(' for ', '.{1,5}'), ministers_found))) + ')',\n",
    "            para.lower()\n",
    "        )\n",
    "        if not minister_match:\n",
    "            minister_match = re.search(\n",
    "            '(' + '|'.join(list(map(lambda s: s.lower().replace(' for ', '(\\s)*(for|of)(\\s)*'), ministers_found))) + ')',\n",
    "            para.lower()\n",
    "        )\n",
    "        if not minister_match:\n",
    "            for existing_minister in ministers_found:\n",
    "                if existing_minister.replace(' ', '') in para.replace(' ', ''):\n",
    "                    minister_match = re.search('(\\s)?'.join(c for c in existing_minister.replace(' ', '').lower()), para.lower())\n",
    "                    break\n",
    "        minister = levenshtein_best_match(minister_match.group(), ministers_found)\n",
    "        print(f'found minister: {str(minister_match.group())}; matched to {minister}')\n",
    "    else:\n",
    "        minister = para[:minister_match.span()[1]].replace(' of ', ' for ')\n",
    "        ministers_found.add(minister)\n",
    "\n",
    "    para = para.replace(minister, '').strip()\n",
    "\n",
    "    if ' and Leader' in minister:\n",
    "        minister = minister[:-11]\n",
    "\n",
    "    return minister, para\n",
    "\n",
    "def get_ministers_and_question(para):\n",
    "    askee, question = extract_first_ministers(para)\n",
    "    \n",
    "    if not re.search('and (the )?Minister', askee):\n",
    "        return (askee,), question   \n",
    "    else:\n",
    "        askee = askee.replace('and the Minister', 'and Minister')\n",
    "        askees = askee.split(' and Minister')\n",
    "        return (askees[0], 'Minister' + askees[1]), question\n",
    "\n",
    "def soup_to_pqs(soup, file):\n",
    "    # print(file)\n",
    "    # seems to happen quite often sadly\n",
    "    if soup.get_text() == '':\n",
    "        print(f'empty text {file}')\n",
    "        return\n",
    "    \n",
    "    stripped_strings = list(map(\n",
    "        lambda text: re.sub(r'\\s+', ' ', text),\n",
    "        filter(\n",
    "            lambda text: not re.match(r'Page:\\s+\\d+', text) and not re.match(r'Column:\\s+\\d+', text),\n",
    "            [text for text in soup.stripped_strings])))\n",
    "    if len(stripped_strings) < 20: # the table at the top of the page alr accounts for most of this.\n",
    "        return\n",
    "    parl_no = int(stripped_strings[4])\n",
    "    sess_no = int(stripped_strings[6])\n",
    "    vol_no = int(stripped_strings[8])\n",
    "    sitting_no = int(stripped_strings[10])\n",
    "    sitting_date = datetime.strptime(stripped_strings[12], '%d-%m-%Y')\n",
    "    section_name_raw = stripped_strings[14].lower()\n",
    "    \n",
    "    if 'answered' in section_name_raw:\n",
    "        section_name = ReportSection.WRITTEN_NA\n",
    "    elif 'written' in section_name_raw:\n",
    "        section_name = ReportSection.WRITTEN\n",
    "    elif 'oral' in section_name_raw:\n",
    "        section_name = ReportSection.ORAL\n",
    "    elif 'budget' in section_name_raw:\n",
    "        section_name = ReportSection.BUDGET\n",
    "    else:\n",
    "        raise f'no section name??? {section_name_raw}'\n",
    "    \n",
    "    title = stripped_strings[16]\n",
    "    the_rest = stripped_strings[20:]\n",
    "    \n",
    "    if section_name == ReportSection.BUDGET:\n",
    "        return # TODO TODO TODOTODOTODO TODO TODO ============================================================================\n",
    "    \n",
    "    while len(the_rest) > 0 and not re.match(r'\\d\\d?', the_rest[0]):\n",
    "        the_rest = the_rest[1:]\n",
    "        \n",
    "    if len(the_rest) == 0:\n",
    "        return\n",
    "        \n",
    "    indices_corresponding_to_pqs = []\n",
    "    indices_corresponding_to_speakers = []\n",
    "    maybe_more_pqs = True\n",
    "    for i in range(len(the_rest)):\n",
    "        if the_rest[i][0] == ':' or (i-1 >= 0 and the_rest[i-1][-1] == ':' and the_rest[i-1] in list(map(lambda s: s.get_text().strip(), soup.select('strong')))): # edge case: (sprs3topic_reportid=oral-answer-2239.html), Ong Ye Kung's first response has the colon bolded, whereas it's normally not bolded. this throws us off. extra check in the condition is to resolve this.\n",
    "            actual_index_to_append = i-1\n",
    "            # edge case: (sprs3topic_reportid=oral-answer-1632.html), \"The Senior Minister of State for Home Affairs (Mr Desmond Lee) (for the Minister for Home Affairs)\" is broken up into multiple entries for some reason. this loop is to ensure the full name and title gets saved.\n",
    "            while the_rest[actual_index_to_append][0] == '(' and the_rest[actual_index_to_append][-1] == ')':\n",
    "                actual_index_to_append -= 1\n",
    "            # edge case: (sprs3topic_reportid=oral-answer-2760.html), \"The Minister of State for Home Affairs (Mr Desmond Tan) (for the  Minister for Home Affairs)\" is also cut in the middle for some reason zzz\n",
    "            bracket_count = the_rest[actual_index_to_append].count('(') - the_rest[actual_index_to_append].count(')')\n",
    "            while bracket_count != 0:\n",
    "                actual_index_to_append -= 1\n",
    "                bracket_count += the_rest[actual_index_to_append].count('(') - the_rest[actual_index_to_append].count(')')\n",
    "            indices_corresponding_to_speakers.append(actual_index_to_append)\n",
    "            maybe_more_pqs = False\n",
    "        elif re.match(r'\\d\\d?', the_rest[i]) and maybe_more_pqs:\n",
    "            indices_corresponding_to_pqs.append(i)\n",
    "            \n",
    "    if len(indices_corresponding_to_pqs) == 0:\n",
    "        print(f'no pqs? {file}')\n",
    "        return\n",
    "    if len(indices_corresponding_to_speakers) == 0:\n",
    "        print(f'no speakers? {file}')\n",
    "        return\n",
    "        \n",
    "    pq_sublists = []\n",
    "    pq_qn_indices = []\n",
    "    while len(indices_corresponding_to_pqs) > 1:\n",
    "        pq_qn_indices.append(the_rest[indices_corresponding_to_pqs[0]])\n",
    "        pq_sublists.append(the_rest[indices_corresponding_to_pqs[0]+1:indices_corresponding_to_pqs[1]])\n",
    "        indices_corresponding_to_pqs = indices_corresponding_to_pqs[1:]\n",
    "        \n",
    "    pq_qn_indices.append(the_rest[indices_corresponding_to_pqs[0]])\n",
    "    pq_sublists.append(the_rest[indices_corresponding_to_pqs[0]+1:indices_corresponding_to_speakers[0]])\n",
    "    \n",
    "    speaking_sublists = []\n",
    "    \n",
    "    while len(indices_corresponding_to_speakers) > 1:\n",
    "        speaking_sublists.append(the_rest[indices_corresponding_to_speakers[0]:indices_corresponding_to_speakers[1]])\n",
    "        indices_corresponding_to_speakers = indices_corresponding_to_speakers[1:]\n",
    "    \n",
    "    speaking_sublists.append(the_rest[indices_corresponding_to_speakers[0]:])\n",
    "    \n",
    "    new_pqs = []\n",
    "    new_pq_indices = []\n",
    "    \n",
    "    for pq_i, sl in zip(pq_qn_indices, pq_sublists):\n",
    "        pq_para = ' '.join(sl)\n",
    "        \n",
    "        #pq_para = trim_off_non_pq_content_at_start(pq_para) # sometimes we end up mistaking other numbers in the text as being the pq numbers. so we deal w that here.\n",
    "        if ' asked the ' not in pq_para:\n",
    "            continue\n",
    "        asker_honorific_name, pq_para = pq_para.split(' asked the ', 1)    \n",
    "        ministers, question = get_ministers_and_question(pq_para)\n",
    "\n",
    "        if question[0] == ',':\n",
    "            question = question[1:].strip()\n",
    "        if len(asker_honorific_name.strip()) == 0:\n",
    "            return\n",
    "        asker, asker_party, asker_parls = honorific_name_to_mp_data(asker_honorific_name.strip())\n",
    "        new_pq_indices.append(int(pq_i))\n",
    "        new_pqs.append([asker, asker_party[0], asker_parls, ministers, question, parl_no, sess_no, vol_no, sitting_no, sitting_date, section_name, title])\n",
    "    \n",
    "    speakers_and_spokens = []\n",
    "    for sl in speaking_sublists:\n",
    "        text = ' '.join(sl)\n",
    "        speaker, spoken = text.split(':', 1)\n",
    "        speaker = speaker.strip()\n",
    "        spoken = spoken.strip()\n",
    "        while len(spoken) > 0 and not spoken[0].isalpha():\n",
    "            spoken = spoken[1:]\n",
    "        if len(spoken) == 0: # edge case (sprs3topic_reportid=written-answer-4142.html). sometimes people are just lost for words i guess.\n",
    "            return\n",
    "        if spoken[:len('Question No')] == 'Question No':\n",
    "            continue\n",
    "        speaker = re.sub(f'\\(for the .*\\)', '', speaker)\n",
    "        speaker = re.sub(f'\\(on behalf of the .*\\)', '', speaker)\n",
    "        if len(speakers_and_spokens) == 0:\n",
    "            speaker_title_match = re.search(minister_regex, speaker)\n",
    "            if not speaker_title_match:\n",
    "                responder_title = ''\n",
    "                if section_name == ReportSection.ORAL: # only oral answers include speaker title. \n",
    "                    print(f'no title for this speaker {speaker} in this file {file}')\n",
    "            else:\n",
    "                responder_title = speaker_title_match.group()\n",
    "        speaker = re.sub(minister_regex, '', speaker)\n",
    "        in_bracket_honorific_match = re.search(f'\\({honorific_regex}.+\\)', speaker.lower())\n",
    "        if in_bracket_honorific_match:\n",
    "            honorific_match_group = in_bracket_honorific_match.group()[1:-1]\n",
    "            if len(honorific_match_group) == 0:\n",
    "                return\n",
    "            speaker = honorific_name_to_mp_data(honorific_match_group)[0]\n",
    "        else:\n",
    "            speaker = re.sub(f'\\(.+\\)', '', speaker)\n",
    "            if len(speaker) == 0:\n",
    "                return\n",
    "            speaker = honorific_name_to_mp_data(speaker)[0]\n",
    "        speakers_and_spokens.append([speaker, spoken])\n",
    "        \n",
    "    assert len(new_pqs) == len(new_pq_indices)\n",
    "        \n",
    "    if len(new_pqs) == 0:\n",
    "        return\n",
    "        \n",
    "    # only 1 pq was asked. so everything else in the file must be related to that pq.\n",
    "    if len(new_pqs) == 1:\n",
    "        responder, response = speakers_and_spokens[0]\n",
    "        new_pqs[0].append(responder)\n",
    "        new_pqs[0].append(responder_title)\n",
    "        new_pqs[0].append(response)\n",
    "        follow_ups = speakers_and_spokens[1:]\n",
    "        new_pqs[0].append(follow_ups)\n",
    "        pqs.append(new_pqs[0])\n",
    "        return\n",
    "    \n",
    "    if len(speakers_and_spokens) == 0:\n",
    "        print(f'found file with no responses {file}')\n",
    "        for new_pq in new_pqs:\n",
    "            new_pq.append('')\n",
    "            new_pq.append('')\n",
    "            new_pq.append('')\n",
    "            new_pq.append([])\n",
    "            pqs.append(new_pq)\n",
    "        return\n",
    "    \n",
    "    # only 1 person spoke after all the pqs were asked. so this person must be responding to all the pqs.\n",
    "    if len(speakers_and_spokens) == 1:\n",
    "        responder, response = speakers_and_spokens[0]\n",
    "        for new_pq in new_pqs:\n",
    "            new_pq.append(responder)\n",
    "            new_pq.append(responder_title)\n",
    "            new_pq.append(response)\n",
    "            new_pq.append([]) # no follow-ups after main response\n",
    "            pqs.append(new_pq)\n",
    "        return\n",
    "\n",
    "    # past this point, the html file has more than one pq, and more than one response to those pqs. \n",
    "    assert len(new_pqs) > 1\n",
    "    assert len(speakers_and_spokens) > 1\n",
    "    \n",
    "    relevant_followups = dict() # stores which spoken thing is relevant to which pq\n",
    "    for new_pq_i in new_pq_indices:\n",
    "        relevant_followups[new_pq_i] = []\n",
    "    first_responder, first_response = speakers_and_spokens[0]\n",
    "    \n",
    "    # usually the responder responds to all the qns at once. we just wanna confirm that.\n",
    "    qn_indices_covered_by_first_response = set()\n",
    "    range_matches = re.findall(r'\\d+ to \\d+', first_response) # e.g. \"i wanna cover qns x to y and z to w\"\n",
    "    if range_matches and len(range_matches) > 0:\n",
    "        for range_match in range_matches:\n",
    "            qn_indices_covered_by_first_response = qn_indices_covered_by_first_response.union(set(range(*list(map(int, range_match.split(' to '))))))\n",
    "    range_matches = re.findall(r'\\d+( )?-( )?\\d+', first_response) # e.g. \"i wanna cover qns x-y and z-w\"\n",
    "    if range_matches and len(range_matches) > 0:\n",
    "        for range_match in range_matches:\n",
    "            qn_indices_covered_by_first_response = qn_indices_covered_by_first_response.union(set(range(*list(map(int, range_match.split('-'))))))\n",
    "    qn_indices_covered_by_first_response = qn_indices_covered_by_first_response.union(set(map(int, re.findall(r'\\d+', first_response))))\n",
    "    if set(new_pq_indices).issubset(qn_indices_covered_by_first_response) or 'all' in first_response or 'together' in first_response or 'every' in first_response:\n",
    "        # the first responder is responding to all the questions at once. \n",
    "        for pq_i in new_pq_indices:\n",
    "            # speakers_and_spokens[0] is asking for permission to answer all qns at once. speakers_and_spokens[1] is the actual response.\n",
    "            relevant_followups[pq_i].append(speakers_and_spokens[1])\n",
    "        speakers_and_spokens = speakers_and_spokens[2:]\n",
    "    else:\n",
    "        print(\"i genuinely don't think we'll reach this point. but if we do, find out which questions this current response is addressing\")\n",
    "        print('rmb to remove the consumed entries from speakers_and_spokens')\n",
    "        print(f'file is {file}')\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "\n",
    "    # no follow-ups after main response\n",
    "    if len(speakers_and_spokens) == 0:\n",
    "        for new_pq_i, new_pq in zip(new_pq_indices, new_pqs):\n",
    "            new_pq.append(relevant_followups[new_pq_i][0][0]) # responder\n",
    "            new_pq.append(responder_title)\n",
    "            new_pq.append(relevant_followups[new_pq_i][0][1]) # response\n",
    "            new_pq.append([])\n",
    "            pqs.append(new_pq)\n",
    "        return\n",
    "    \n",
    "    # gotta map everything that is said, to the relevant pqs. \n",
    "    asker_to_new_pqi = dict()\n",
    "    for new_pq_i, new_pq in zip(new_pq_indices, new_pqs):\n",
    "        asker_to_new_pqi[new_pq[0]] = new_pq_i\n",
    "\n",
    "    pqs_with_new_responses_since_last_time_first_responder_spoke = set()\n",
    "    pqs_that_first_responder_covered_with_last_reply = set(new_pq_indices)\n",
    "    \n",
    "    for i in range(len(speakers_and_spokens)):\n",
    "        speaker, spoken = speakers_and_spokens[i]\n",
    "        #print(f'pqs w new responses: {pqs_with_new_responses_since_last_time_first_responder_spoke}')\n",
    "        #print(f'pqs last covered: {pqs_that_first_responder_covered_with_last_reply}')\n",
    "        \n",
    "        if speaker in asker_to_new_pqi.keys(): # something is said by someone who asked a pq. so it's relevant to that pq. \n",
    "            #print(f'said by someone who asked a pq. {asker_to_new_pqi[speaker]} {[speaker, spoken[:100]]}')\n",
    "            relevant_followups[asker_to_new_pqi[speaker]].append([speaker, spoken])\n",
    "            pqs_with_new_responses_since_last_time_first_responder_spoke.add(asker_to_new_pqi[speaker])\n",
    "            continue\n",
    "        \n",
    "        # something is said by someone who did not ask any pqs. \n",
    "        \n",
    "        if speaker == first_responder:\n",
    "            # it's the original responder, ofc responding to some followup qns. \n",
    "            # find out which qns these are by seeing who said stuff since the last time this guy spoke.\n",
    "            #print(f'said by responder. adding to pqs: {pqs_with_new_responses_since_last_time_first_responder_spoke}. {[speaker, spoken[:100]]}')\n",
    "            for pq_with_new_response in pqs_with_new_responses_since_last_time_first_responder_spoke:\n",
    "                relevant_followups[pq_with_new_response].append([speaker, spoken])\n",
    "            pqs_that_first_responder_covered_with_last_reply = pqs_with_new_responses_since_last_time_first_responder_spoke\n",
    "            pqs_with_new_responses_since_last_time_first_responder_spoke = set()\n",
    "            continue\n",
    "        else:\n",
    "            # otherwise, it's a follow-up qn from someone originally unrelated. naturally the qn would be targeted at the first responder, and the\n",
    "            # content has to be rel8ed to whatever the responder last said. so we assign it to the same set of pqs.\n",
    "            #print(f'new followup qn. adding to pqs: {pqs_that_first_responder_covered_with_last_reply}. {[speaker, spoken[:100]]}')\n",
    "            for pq_covered in pqs_that_first_responder_covered_with_last_reply:\n",
    "                relevant_followups[pq_covered].append([speaker, spoken])\n",
    "                pqs_with_new_responses_since_last_time_first_responder_spoke.add(pq_covered)\n",
    "            continue\n",
    "    \n",
    "    for new_pq_i, new_pq in zip(new_pq_indices, new_pqs):\n",
    "        new_pq.append(relevant_followups[new_pq_i][0][0]) # responder\n",
    "        new_pq.append(responder_title)\n",
    "        new_pq.append(relevant_followups[new_pq_i][0][1]) # response\n",
    "        new_pq.append(relevant_followups[new_pq_i][1:])\n",
    "        pqs.append(new_pq)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0314366-9a4d-4d9e-b74d-9fd5ebfd7830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprs3topic_reportid=oral-answer-2760.html\n",
      "levenshtein matched mr speaker to amrin amin\n",
      "0/1\n",
      "=====DONE==================================================\n",
      "total pqs: 1\n",
      "total files: 1\n",
      "avg pqs per file: 1.0\n",
      "files with exceptions: []\n"
     ]
    }
   ],
   "source": [
    "pqs = []\n",
    "files_and_exceptions = []\n",
    "files_to_run_through = os.listdir('scraped_content')[500:550]\n",
    "\n",
    "for i in range(len(files_to_run_through)):\n",
    "    file = files_to_run_through[i]\n",
    "    print(file)\n",
    "    filepath = os.path.join('scraped_content', file)\n",
    "    if os.stat(filepath).st_size < 200000: # the html elements alr take up more than 300kb, so if a file is this small then someth's wrong\n",
    "        continue\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "        soup_to_pqs(soup, file)\n",
    "    except Exception as e:\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        #raise e\n",
    "        print(f'exception: {str(e)} - {file}')\n",
    "        files_and_exceptions.append([file, e])\n",
    "        continue\n",
    "        \n",
    "    if i%25==0:\n",
    "        print(f'{i}/{len(files_to_run_through)}')\n",
    "        \n",
    "print('=====DONE==================================================')\n",
    "print(f'total pqs: {len(pqs)}')\n",
    "print(f'total files: {len(files_to_run_through)}')\n",
    "print(f'avg pqs per file: {len(pqs)/len(files_to_run_through)}')\n",
    "print(f'files with exceptions: {files_and_exceptions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "883514f5-2bfa-4845-b83e-3d4cfbb4c88e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asker_name</th>\n",
       "      <th>asker_party</th>\n",
       "      <th>asker_parliaments</th>\n",
       "      <th>askees</th>\n",
       "      <th>question</th>\n",
       "      <th>parliament_no</th>\n",
       "      <th>session_no</th>\n",
       "      <th>volume_no</th>\n",
       "      <th>sitting_no</th>\n",
       "      <th>sitting_date</th>\n",
       "      <th>report_section</th>\n",
       "      <th>title</th>\n",
       "      <th>responder_name</th>\n",
       "      <th>responder_title</th>\n",
       "      <th>response</th>\n",
       "      <th>discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yip Hon Weng</td>\n",
       "      <td>People's Action Party</td>\n",
       "      <td>(14,)</td>\n",
       "      <td>(Minister for Home Affairs,)</td>\n",
       "      <td>(a) for the last three years, what is the annu...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>53</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>ReportSection.ORAL</td>\n",
       "      <td>Annual Number of Job Scam Victims Aged 60 and ...</td>\n",
       "      <td>Desmond Tan</td>\n",
       "      <td>The Minister of State for Home Affairs</td>\n",
       "      <td>Mr Speaker, from 2019 to 2021, there were 4,72...</td>\n",
       "      <td>[[Amrin Amin, Mr Yip Hon Weng.], [Yip Hon Weng...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asker_name            asker_party asker_parliaments  \\\n",
       "0  Yip Hon Weng  People's Action Party             (14,)   \n",
       "\n",
       "                         askees  \\\n",
       "0  (Minister for Home Affairs,)   \n",
       "\n",
       "                                            question  parliament_no  \\\n",
       "0  (a) for the last three years, what is the annu...             14   \n",
       "\n",
       "   session_no  volume_no  sitting_no sitting_date      report_section  \\\n",
       "0           1         95          53   2022-03-03  ReportSection.ORAL   \n",
       "\n",
       "                                               title responder_name  \\\n",
       "0  Annual Number of Job Scam Victims Aged 60 and ...    Desmond Tan   \n",
       "\n",
       "                          responder_title  \\\n",
       "0  The Minister of State for Home Affairs   \n",
       "\n",
       "                                            response  \\\n",
       "0  Mr Speaker, from 2019 to 2021, there were 4,72...   \n",
       "\n",
       "                                          discussion  \n",
       "0  [[Amrin Amin, Mr Yip Hon Weng.], [Yip Hon Weng...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_df = pd.DataFrame(pqs, columns=['asker_name', 'asker_party', 'asker_parliaments', 'askees', 'question', 'parliament_no', 'session_no', 'volume_no', 'sitting_no', 'sitting_date', 'report_section', 'title', 'responder_name', 'responder_title', 'response', 'discussion'])\n",
    "pq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b10be6-e33f-4d37-9194-0f2c9936a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_df.to_csv('pqs.csv', index=False, sep='|') # sep=',' gives formatting issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cda24a8-34bc-42ad-bc1f-6dc220aae435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parties: {\"People's Action Party\"} (len: 1)\n",
      "\n",
      "askees: {'Minister for Home Affairs'} (len: 1)\n"
     ]
    }
   ],
   "source": [
    "assert all(pq_df.parliament_no < 15) and all(pq_df.parliament_no >= 12)\n",
    "assert all(map(lambda x: not x[0].isupper(), pq_df.question.values)) \n",
    "assert all(map(lambda x: not x[:3] == 'and', pq_df.question.values)) \n",
    "parties_set = set(pq_df.asker_party.values)\n",
    "print(f'parties: {parties_set} (len: {len(parties_set)})')\n",
    "print()\n",
    "askee_set = set([askees for sublist in pq_df.askees for askees in sublist])\n",
    "print(f'askees: {askee_set} (len: {len(askee_set)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b90b30e-8e35-4dfb-93f2-b5d381b26033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(pq_df.asker_name.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c547b906-6b9c-4934-ac3d-9b8c3a345c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'Yip Hon Weng', 1, 'Yip Hon Weng', 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = 999\n",
    "min_mp = None\n",
    "max_count = 0\n",
    "max_mp = None\n",
    "less_than_ten = 0\n",
    "just_one = 0\n",
    "for name in set(pq_df.asker_name.values):\n",
    "    count_here = pq_df[pq_df.asker_name == name]['asker_name'].count()\n",
    "    if count_here < min_count:\n",
    "        min_count = count_here\n",
    "        min_mp = name\n",
    "    if count_here > max_count:\n",
    "        max_count = count_here\n",
    "        max_mp = name\n",
    "    if count_here < 10:\n",
    "        less_than_ten += 1\n",
    "    if count_here == 1:\n",
    "        just_one += 1\n",
    "        \n",
    "min_count, min_mp, max_count, max_mp, less_than_ten, just_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb69906-c6ec-4e0d-a812-a174eb892238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
