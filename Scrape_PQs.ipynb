{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this nb is responsible for scraping the data and handling all the messy html stuff. gives you a csv file containing the pq paragraphs and a bit of metadata.\n",
    "\n",
    "sometimes code is too fast for browser driver. in such cases, try increasing sleep time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "import math\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import os\n",
    "from enum import Enum\n",
    "import re\n",
    "import pandas as pd\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportSection(Enum):\n",
    "    WRITTEN = 'Written Answers to Questions'\n",
    "    WRITTEN_NA = 'Written Answers to Questions for Oral Answer Not Answered by End of Question Time'\n",
    "    ORAL = 'Oral Answers to Questions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'chromedriver_win32/chromedriver.exe'\n",
    "SEARCH_TERM = 'asked the' # can be blank\n",
    "# PARLIAMENT_INDEX = -1 # legislative assembly = 1, 1st parliament = 2, etc etc. -1 to ignore this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_completely_viewable(driver, elem):\n",
    "    elem_left_bound = elem.location.get('x')\n",
    "    elem_top_bound = elem.location.get('y')\n",
    "    elem_width = elem.size.get('width')\n",
    "    elem_height = elem.size.get('height')\n",
    "    elem_right_bound = elem_left_bound + elem_width\n",
    "    elem_lower_bound = elem_top_bound + elem_height\n",
    "\n",
    "    win_upper_bound = driver.execute_script('return window.pageYOffset')\n",
    "    win_left_bound = driver.execute_script('return window.pageXOffset')\n",
    "    win_width = driver.execute_script('return document.documentElement.clientWidth')\n",
    "    win_height = driver.execute_script('return document.documentElement.clientHeight')\n",
    "    win_right_bound = win_left_bound + win_width\n",
    "    win_lower_bound = win_upper_bound + win_height\n",
    "\n",
    "    return all((win_left_bound <= elem_left_bound,\n",
    "                win_right_bound >= elem_right_bound,\n",
    "                win_upper_bound <= elem_top_bound,\n",
    "                win_lower_bound >= elem_lower_bound)\n",
    "              )\n",
    "\n",
    "def find_element(driver, elem):\n",
    "    y_height = 0\n",
    "    while not element_completely_viewable(driver, elem):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {y_height})\") \n",
    "        y_height += 100\n",
    "        time.sleep(0.5)\n",
    "\n",
    "def find_and_click(driver, elem):\n",
    "    find_element(driver, elem)\n",
    "    elem.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\AppData\\Local\\Temp\\ipykernel_21904\\1286528577.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(DRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search parameters submitted.\n",
      "11904 results found spanning 596 page(s)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(DRIVER_PATH) \n",
    " \n",
    "page_url = 'https://sprs.parl.gov.sg/search/home' \n",
    "driver.get(page_url) \n",
    "driver.maximize_window() \n",
    "driver.implicitly_wait(20) \n",
    "time.sleep(2) \n",
    " \n",
    "# Get search box and fill it up \n",
    "\n",
    "search = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[1]/div/div[1]/input')\n",
    "search.send_keys(SEARCH_TERM) \n",
    "\n",
    "# exact phrase search\n",
    "\n",
    "search_opt = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[2]/div/div[1]/select')\n",
    "find_and_click(driver, search_opt)\n",
    "exact_search_opt = driver.find_element(by=By.XPATH, value='//*[@id=\"divmpscreen2\"]/div[2]/div[2]/div/div[1]/select/option[3]')\n",
    "find_and_click(driver, exact_search_opt)\n",
    " \n",
    "# Uncomment following two lines to only search in titles # \n",
    "#checkbox = driver.find_element_by_xpath('//*[@id=\"divmpscreen2\"]/div[2]/div[1]/div/label/input')\n",
    "#checkbox.click() \n",
    "\n",
    "# from 1st parliament to 14th. ignore legis. assembly cuz the parl website has no list of members.\n",
    "for i in range(2, 16):\n",
    "    session = driver.find_element(\n",
    "        by=By.XPATH, \n",
    "        value=f'/html/body/app-root/app-search/div/div[2]/div[2]/div[1]/div/div[3]/select/option[{i}]')\n",
    "    session.click() \n",
    " \n",
    "# advanced search - only look for oral and written answers to questions\n",
    "\n",
    "adv_search_opt = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[3]/div/div[1]/h5/a')\n",
    "find_and_click(driver, adv_search_opt)\n",
    "section_selection = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[3]/div/div[2]/div/div/div/div/div[3]/div/select')\n",
    "find_element(driver, section_selection)\n",
    "for selection_option_index in [13, 19, 20]: # corresponds to answers to questions    \n",
    "    option = driver.find_element(\n",
    "        by=By.XPATH, \n",
    "        value=f'/html/body/app-root/app-search/div/div[3]/div/div[2]/div/div/div/div/div[3]/div/select/option[{selection_option_index}]')\n",
    "    option.click()\n",
    "\n",
    "\n",
    "# Find submit element and press enter\n",
    "\n",
    "submit = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[3]/div/button[2]') \n",
    "submit.send_keys(Keys.RETURN) \n",
    "\n",
    "print('Search parameters submitted.') \n",
    "\n",
    "# Switch window and check for number of search results \n",
    "\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "driver.implicitly_wait(20) \n",
    "time.sleep(2) \n",
    "\n",
    "try:\n",
    "    num_res_label = driver.find_element(by=By.CLASS_NAME, value='showingResults')\n",
    "except: \n",
    "    print('Error when finding num_res_label')\n",
    "    \n",
    "num_res_text = num_res_label.get_attribute('innerHTML')\n",
    "    \n",
    "if 'No' in num_res_text:\n",
    "    print('no results found') # but site is quite erratic, may take a few tries to get results (idk why)\n",
    "else:\n",
    "    num_results = int(num_res_text[num_res_text.rfind(' ')+1:])\n",
    "    num_pages = math.ceil(num_results / 20)\n",
    "    print(f'{num_results} results found spanning {num_pages} page(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 / 11904 docs at 1520\n",
      "Seen 1 / 596 pages\n",
      "Saved 40 / 11904 docs at 1522\n",
      "Seen 2 / 596 pages\n",
      "Saved 60 / 11904 docs at 1524\n",
      "Seen 3 / 596 pages\n",
      "Saved 80 / 11904 docs at 1526\n",
      "Seen 4 / 596 pages\n",
      "Saved 100 / 11904 docs at 1528\n",
      "Seen 5 / 596 pages\n",
      "Saved 120 / 11904 docs at 1530\n",
      "Seen 6 / 596 pages\n",
      "Saved 140 / 11904 docs at 1532\n",
      "Seen 7 / 596 pages\n",
      "Saved 160 / 11904 docs at 1533\n",
      "Seen 8 / 596 pages\n"
     ]
    }
   ],
   "source": [
    "docs_saved = 0\n",
    "pages_seen = 0\n",
    "\n",
    "pq_paras = []\n",
    "\n",
    "while (True):\n",
    "    driver.switch_to.window(driver.window_handles[1])                  \n",
    "\n",
    "    elems = driver.find_elements(by=By.XPATH, value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/table/tbody')\n",
    "    links = list(map(lambda elem: elem.find_elements(by=By.XPATH, value='.//tr[1]/td[2]/a')[0], elems))\n",
    "    \n",
    "    for link in links:\n",
    "        find_and_click(driver, link)\n",
    "        \n",
    "        # Switch to page with content and get URL name\n",
    "        time.sleep(1)\n",
    "        driver.switch_to.window(driver.window_handles[2])         \n",
    "        item_key = driver.current_url.split('/')[-1]         \n",
    "        item_key = item_key.replace('?', '_') # Replace ? because it would be an invalid filename \n",
    "\n",
    "        file = item_key\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        if file[0:4] == 'sprs':\n",
    "            is_sprs = True\n",
    "        else:\n",
    "            is_sprs = False\n",
    "            \n",
    "        soup = bs(page_source, 'html.parser')\n",
    "\n",
    "        if is_sprs:\n",
    "            # extract metadata from the table at the top of the page\n",
    "            topic_table = soup.find('table', {'class': 'topic'})\n",
    "            topic_text = topic_table.get_text()\n",
    "\n",
    "            parl_no_label_span = re.match(r'Parliament No:', topic_text).span()\n",
    "            topic_text = topic_text[parl_no_label_span[1]:]\n",
    "            parl_no = int(re.match(r'\\d+', topic_text).group())\n",
    "\n",
    "            sit_date_label_span = re.match(r'.*Sitting Date:', topic_text).span()\n",
    "            topic_text = topic_text[sit_date_label_span[1]:]\n",
    "            sit_date = datetime.strptime(re.match(r'\\d+-\\d+-\\d+', topic_text).group(), '%d-%m-%Y')\n",
    "\n",
    "            # extract report section via filename\n",
    "            if 'oral-answer' in file:\n",
    "                section = ReportSection.ORAL\n",
    "            elif 'written-answer-na' in file:\n",
    "                section = ReportSection.WRITTEN_NA\n",
    "            elif 'written-answer' in file:\n",
    "                section = ReportSection.WRITTEN\n",
    "            else:\n",
    "                raise 'unidentified section (sprs)'\n",
    "            section = section.value\n",
    "\n",
    "            # extract paragraphs from report in the form of rows of table\n",
    "            report_table = soup.find('div', {'class': 'reportTable'})\n",
    "            report_table_rows = report_table.findChildren()\n",
    "\n",
    "            # find out which rows correspond to pqs and which rows don't.\n",
    "            for row in report_table_rows:\n",
    "                paragraph = row.get_text()\n",
    "                paragraph = paragraph.replace('\\xa0', ' ')\n",
    "                if not re.match(r'\\d+.{8,75} asked the .+', paragraph):\n",
    "                    continue\n",
    "                paragraph_no_num = paragraph[\n",
    "                    re.search('\\d+', paragraph).span()[1]:]\n",
    "                pq_paras.append((paragraph_no_num.strip(), sit_date, parl_no, section))\n",
    "        else:\n",
    "            # extract metadata from meta elements directly\n",
    "            sit_date_str = soup.find('meta', {'name': 'Sit_Date'})['content']\n",
    "            parl_no_str = soup.find('meta', {'name': 'Parl_No'})['content']\n",
    "            section_str = soup.find('meta', {'name': 'Sect_Name'})['content']\n",
    "\n",
    "            sit_date = datetime.strptime(sit_date_str, '%Y-%m-%d')\n",
    "            parl_no = int(parl_no_str)\n",
    "            if 'NOT REACHED' in section_str:\n",
    "                section = ReportSection.WRITTEN_NA\n",
    "            elif 'ORAL' in section_str:\n",
    "                section = ReportSection.ORAL\n",
    "            elif 'WRITTEN' in section_str:\n",
    "                section = ReportSection.WRITTEN\n",
    "            else:\n",
    "                raise 'unidentified section (non sprs)'\n",
    "            section = section.value\n",
    "\n",
    "            # extract paragraphs from table in the form of raw text (since each paragraph doesn't have its own element)\n",
    "            report_table = soup.find('div', {'class': 'reportTable'})\n",
    "            raw_text = report_table.get_text()\n",
    "\n",
    "            # look for things like 1. 2. etc\n",
    "            first_number_dot_occurrence = re.search(r'\\d+\\. .{8,75} asked the ', raw_text)\n",
    "\n",
    "            if not first_number_dot_occurrence:\n",
    "                continue\n",
    "\n",
    "            # cut off everything before the actual body of the text\n",
    "            raw_text = raw_text[first_number_dot_occurrence.span()[0]:]\n",
    "\n",
    "            # formatting stuff\n",
    "            raw_text = raw_text.replace('\\n\\n', ' ')\n",
    "            raw_paras = raw_text.split('\\xa0\\xa0\\xa0\\xa0')\n",
    "\n",
    "            new_pq_paras = list(\n",
    "                map(lambda s: (s[re.search('\\d+\\.', s).span()[1]:].strip(), sit_date, parl_no, section), # remove index, add sitting date, parl no, section\n",
    "                    map(lambda s: re.sub('Column: \\d+', '', s.replace('  ', ' ').strip()), # remove column indicators and extra spaces\n",
    "                        filter(lambda para: re.match(r'\\d+\\. .{8,75} asked the ', para), raw_paras)))) # keep only paragraphs that are pqs.\n",
    "\n",
    "            pq_paras += new_pq_paras\n",
    "\n",
    "        docs_saved += 1\n",
    "        if docs_saved % 20 == 0:\n",
    "            print(f'Saved {docs_saved} / {num_results} docs at {datetime.now().strftime(\"%H%M\")}')\n",
    "\n",
    "        # Close tab         \n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if docs_saved == num_results:\n",
    "        break\n",
    "    elif docs_saved == 20: # if we're on the first results page then there is no previous page button\n",
    "        next_page = driver.find_element(\n",
    "            by=By.XPATH, \n",
    "            value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/div[3]/section/ul/li[1]/a')\n",
    "    else: # if we're not on the first results page then there is a previous page button\n",
    "        next_page = driver.find_element(\n",
    "            by=By.XPATH, \n",
    "            value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/div[3]/section/ul/li[3]/a')\n",
    "    \n",
    "    find_and_click(driver, next_page)\n",
    "    pages_seen += 1\n",
    "    print(f'Seen {pages_seen} / {num_pages} pages')\n",
    "\n",
    "    # Sleep momentarily because next page takes a while to load     \n",
    "    time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_paras_df = pd.DataFrame(pq_paras, columns=['para', 'sit_date', 'parl_no', 'section'])\n",
    "pq_paras_df.to_csv('pq_paras.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
