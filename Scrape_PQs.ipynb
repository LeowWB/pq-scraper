{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes code is too fast for browser driver. in such cases, try increasing sleep time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import math\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import os\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'C:/Users/thisi/Documents/GitHub/pq-scraper/chromedriver_win32/chromedriver.exe'\n",
    "SEARCH_TERM = 'asked the' # can be blank\n",
    "PARLIAMENT_INDEX = -1 # legislative assembly = 1, 1st parliament = 2, etc etc. -1 to ignore this field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def element_completely_viewable(driver, elem):\n",
    "    elem_left_bound = elem.location.get('x')\n",
    "    elem_top_bound = elem.location.get('y')\n",
    "    elem_width = elem.size.get('width')\n",
    "    elem_height = elem.size.get('height')\n",
    "    elem_right_bound = elem_left_bound + elem_width\n",
    "    elem_lower_bound = elem_top_bound + elem_height\n",
    "\n",
    "    win_upper_bound = driver.execute_script('return window.pageYOffset')\n",
    "    win_left_bound = driver.execute_script('return window.pageXOffset')\n",
    "    win_width = driver.execute_script('return document.documentElement.clientWidth')\n",
    "    win_height = driver.execute_script('return document.documentElement.clientHeight')\n",
    "    win_right_bound = win_left_bound + win_width\n",
    "    win_lower_bound = win_upper_bound + win_height\n",
    "\n",
    "    return all((win_left_bound <= elem_left_bound,\n",
    "                win_right_bound >= elem_right_bound,\n",
    "                win_upper_bound <= elem_top_bound,\n",
    "                win_lower_bound >= elem_lower_bound)\n",
    "              )\n",
    "\n",
    "def find_and_click(driver, elem):\n",
    "    y_height = 0\n",
    "    while not element_completely_viewable(driver, elem):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {y_height})\") \n",
    "        y_height += 100\n",
    "        time.sleep(0.5)\n",
    "    elem.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\AppData\\Local\\Temp\\ipykernel_21176\\3484506804.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(DRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search parameters submitted.\n",
      "13875 results found spanning 694 page(s)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(DRIVER_PATH) \n",
    " \n",
    "page_url = 'https://sprs.parl.gov.sg/search/home' \n",
    "driver.get(page_url) \n",
    "driver.maximize_window() \n",
    "driver.implicitly_wait(20) \n",
    "time.sleep(2) \n",
    " \n",
    "# Get search box and fill it up \n",
    "\n",
    "search = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[1]/div/div[1]/input')\n",
    "search.send_keys(SEARCH_TERM) \n",
    "\n",
    "# exact phrase search\n",
    "\n",
    "search_opt = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[2]/div/div[1]/select')\n",
    "find_and_click(driver, search_opt)\n",
    "exact_search_opt = driver.find_element(by=By.XPATH, value='//*[@id=\"divmpscreen2\"]/div[2]/div[2]/div/div[1]/select/option[3]')\n",
    "find_and_click(driver, exact_search_opt)\n",
    " \n",
    "# Uncomment following two lines to only search in titles # \n",
    "#checkbox = driver.find_element_by_xpath('//*[@id=\"divmpscreen2\"]/div[2]/div[1]/div/label/input')\n",
    "#checkbox.click() \n",
    "\n",
    "if PARLIAMENT_INDEX != -1:\n",
    "    session = driver.find_element(\n",
    "        by=By.XPATH, \n",
    "        value=f'/html/body/app-root/app-search/div/div[2]/div[2]/div[1]/div/div[3]/select/option[{PARLIAMENT_INDEX}]')\n",
    "    session.click() \n",
    " \n",
    "# Find submit element and press enter\n",
    "\n",
    "submit = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[3]/div/button[2]') \n",
    "submit.send_keys(Keys.RETURN) \n",
    "\n",
    "print('Search parameters submitted.') \n",
    "\n",
    "# Switch window and check for number of search results \n",
    "\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "driver.implicitly_wait(20) \n",
    "time.sleep(2) \n",
    "\n",
    "try:\n",
    "    num_res_label = driver.find_element(by=By.CLASS_NAME, value='showingResults')\n",
    "except: \n",
    "    print('Error when finding num_res_label')\n",
    "    \n",
    "num_res_text = num_res_label.get_attribute('innerHTML')\n",
    "    \n",
    "if 'No' in num_res_text:\n",
    "    print('no results found') # but site is quite erratic, may take a few tries to get results (idk why)\n",
    "else:\n",
    "    num_results = int(num_res_text[num_res_text.rfind(' ')+1:])\n",
    "    num_pages = math.ceil(num_results / 20)\n",
    "    print(f'{num_results} results found spanning {num_pages} page(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 links on this page\n",
      "Saved 1 / 13875 docs\n",
      "Saved 2 / 13875 docs\n",
      "Saved 3 / 13875 docs\n"
     ]
    }
   ],
   "source": [
    "docs_saved = 0\n",
    "pages_seen = 0\n",
    "\n",
    "if not os.path.exists('scraped_content'):\n",
    "    os.makedirs('scraped_content')\n",
    "    \n",
    "while (True):\n",
    "    driver.switch_to.window(driver.window_handles[1])                  \n",
    "\n",
    "    elems = driver.find_elements(by=By.XPATH, value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/table/tbody')\n",
    "    links = list(map(lambda elem: elem.find_elements(by=By.XPATH, value='.//tr[1]/td[2]/a')[0], elems))\n",
    "    \n",
    "    print(f'{len(links)} links on this page')\n",
    "    \n",
    "    for link in links:\n",
    "        find_and_click(driver, link)\n",
    "        \n",
    "        # Switch to page with content and get URL name\n",
    "        time.sleep(1)\n",
    "        driver.switch_to.window(driver.window_handles[2])         \n",
    "        item_key = driver.current_url.split('/')[-1]         \n",
    "        item_key = item_key.replace('?', '_') # Replace ? because it would be an invalid filename \n",
    "\n",
    "        # Write out each page source as a file         \n",
    "        with open('scraped_content/' + item_key + '.txt', encoding = 'utf-8', mode = 'w+') as file:             \n",
    "            file.write(driver.page_source)    \n",
    "        docs_saved += 1\n",
    "        print(f'Saved {docs_saved} / {num_results} docs')\n",
    "\n",
    "        # Close tab         \n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if docs_saved == num_results:\n",
    "        break\n",
    "    elif docs_saved == 20: # if we're on the first results page then there is no previous page button\n",
    "        next_page = driver.find_element(\n",
    "            by=By.XPATH, \n",
    "            value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/div[3]/section/ul/li[1]/a')\n",
    "    else: # if we're not on the first results page then there is a previous page button\n",
    "        next_page = driver.find_element(\n",
    "            by=By.XPATH, \n",
    "            value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/div[3]/section/ul/li[3]/a')\n",
    "    \n",
    "    find_and_click(driver, next_page)\n",
    "    pages_seen += 1\n",
    "    print(f'Seen {pages_seen} / {num_pages} pages')\n",
    "\n",
    "    # Sleep momentarily because next page takes a while to load     \n",
    "    time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some old code from weiling\n",
    "'''\n",
    "# Nested for loop to click through all search results \n",
    "for page in range(num_pages):         \n",
    "    # This assumes that 20 search results are returned, which are 1-indexed in the xpaths     \n",
    "    for item in range (1, 21):\n",
    "        # Switch to search results page         \n",
    "        driver.switch_to.window(driver.window_handles[1])                  \n",
    "        # Get element to click on, to see each individual page with content         \n",
    "        # Last page will have fewer than 20 elements, so need to handle this exception         \n",
    "        try:             \n",
    "            elem = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/table/tbody[{}]/tr[1]/td[2]/a'.format(item))\n",
    "            elem.click()     \n",
    "            \n",
    "            # Switch to page with content and get URL name         \n",
    "            driver.switch_to.window(driver.window_handles[2])         \n",
    "            item_key = driver.current_url.split('/')[-1]         \n",
    "            item_key = item_key.replace('?', '_') # Replace ? because it would be an invalid filename \n",
    "\n",
    "            # Append result to dictionary for later processing         \n",
    "            res_dict[item_key] = driver.page_source  \n",
    "\n",
    "            # Write out each page source as a file         \n",
    "            with open('scraped_content/' + item_key + '.txt', encoding = 'utf-8', mode = 'w+') as file:             \n",
    "                file.write(driver.page_source)                  \n",
    "\n",
    "            # Close tab         \n",
    "            driver.close()              \n",
    "        \n",
    "        except:             \n",
    "            break   \n",
    "                \n",
    "        # Switch back to search results tab     \n",
    "        driver.switch_to.window(driver.window_handles[1])          \n",
    "        \n",
    "        # Click on next page once 20 results have been saved     \n",
    "        # Next page button changes after first 20 results are shown, hence the need for enclosing the element xpath in a try block     \n",
    "        try:         \n",
    "            next_page = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[3]/section/ul/li[3]/a/i')     \n",
    "        except:         \n",
    "            next_page = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[3]/section/ul/li[1]/a/i')     \n",
    "        next_page.click()          \n",
    "        \n",
    "        # Sleep momentarily because next page takes a while to load     \n",
    "        time.sleep(2) \n",
    "\n",
    "# Check that all results are stored \n",
    "assert len(res_dict.keys()) == int(res[-1]), \"It looks like not all the results were stored!\" \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
