{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes code is too fast for browser driver. in such cases, try increasing sleep time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import math\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import os\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'C:/Users/thisi/Documents/GitHub/pq-scraper/chromedriver_win32/chromedriver.exe'\n",
    "SEARCH_TERM = 'enter search term here' # can be blank\n",
    "PARLIAMENT_INDEX = 1 # legislative assembly = 1, 1st parliament = 2, etc etc. -1 to ignore this field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\AppData\\Local\\Temp\\ipykernel_17212\\2453240245.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(DRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search parameters submitted.\n",
      "16 results found spanning 1 page(s)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(DRIVER_PATH) \n",
    " \n",
    "page_url = 'https://sprs.parl.gov.sg/search/home' \n",
    "driver.get(page_url) \n",
    "driver.maximize_window() \n",
    "driver.implicitly_wait(20) \n",
    "time.sleep(2) \n",
    " \n",
    "# Get search box and fill it up \n",
    "\n",
    "search = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[1]/div/div[1]/input')\n",
    "search.send_keys(SEARCH_TERM) \n",
    " \n",
    "# Uncomment following two lines to only search in titles # \n",
    "#checkbox = driver.find_element_by_xpath('//*[@id=\"divmpscreen2\"]/div[2]/div[1]/div/label/input')\n",
    "#checkbox.click() \n",
    "\n",
    "if PARLIAMENT_INDEX != -1:\n",
    "    session = driver.find_element(\n",
    "        by=By.XPATH, \n",
    "        value=f'/html/body/app-root/app-search/div/div[2]/div[2]/div[1]/div/div[3]/select/option[{PARLIAMENT_INDEX}]')\n",
    "    session.click() \n",
    " \n",
    "# Find submit element and press enter\n",
    "\n",
    "submit = driver.find_element(by=By.XPATH, value='/html/body/app-root/app-search/div/div[2]/div[2]/div[3]/div/button[2]') \n",
    "submit.send_keys(Keys.RETURN) \n",
    "\n",
    "print('Search parameters submitted.') \n",
    "\n",
    "# Switch window and check for number of search results \n",
    "\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "driver.implicitly_wait(20) \n",
    "time.sleep(2) \n",
    "\n",
    "try:\n",
    "    num_res_label = driver.find_element(by=By.CLASS_NAME, value='showingResults')\n",
    "except: \n",
    "    print('Error when finding num_res_label')\n",
    "    \n",
    "num_res_text = num_res_label.get_attribute('innerHTML')\n",
    "    \n",
    "if 'No' in num_res_text:\n",
    "    print('no results found') # but site is quite erratic, may take a few tries to get results (idk why)\n",
    "else:\n",
    "    num_results = int(num_res_text[num_res_text.rfind(' ')+1:])\n",
    "    num_pages = math.ceil(num_results / 20)\n",
    "    print(f'{num_results} results found spanning {num_pages} page(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 links on this page\n",
      "Saved 1 / 16 docs\n",
      "Saved 2 / 16 docs\n",
      "Saved 3 / 16 docs\n",
      "Saved 4 / 16 docs\n",
      "Saved 5 / 16 docs\n",
      "Saved 6 / 16 docs\n",
      "Saved 7 / 16 docs\n",
      "Saved 8 / 16 docs\n",
      "Saved 9 / 16 docs\n",
      "Saved 10 / 16 docs\n",
      "Saved 11 / 16 docs\n",
      "Saved 12 / 16 docs\n",
      "Saved 13 / 16 docs\n",
      "Saved 14 / 16 docs\n",
      "Saved 15 / 16 docs\n",
      "Saved 16 / 16 docs\n"
     ]
    }
   ],
   "source": [
    "docs_saved = 0\n",
    "pages_seen = 0\n",
    "\n",
    "if not os.path.exists('scraped_content'):\n",
    "    os.makedirs('scraped_content')\n",
    "    \n",
    "def element_completely_viewable(driver, elem):\n",
    "    elem_left_bound = elem.location.get('x')\n",
    "    elem_top_bound = elem.location.get('y')\n",
    "    elem_width = elem.size.get('width')\n",
    "    elem_height = elem.size.get('height')\n",
    "    elem_right_bound = elem_left_bound + elem_width\n",
    "    elem_lower_bound = elem_top_bound + elem_height\n",
    "\n",
    "    win_upper_bound = driver.execute_script('return window.pageYOffset')\n",
    "    win_left_bound = driver.execute_script('return window.pageXOffset')\n",
    "    win_width = driver.execute_script('return document.documentElement.clientWidth')\n",
    "    win_height = driver.execute_script('return document.documentElement.clientHeight')\n",
    "    win_right_bound = win_left_bound + win_width\n",
    "    win_lower_bound = win_upper_bound + win_height\n",
    "\n",
    "    return all((win_left_bound <= elem_left_bound,\n",
    "                win_right_bound >= elem_right_bound,\n",
    "                win_upper_bound <= elem_top_bound,\n",
    "                win_lower_bound >= elem_lower_bound)\n",
    "              )\n",
    "\n",
    "while (True):\n",
    "    driver.switch_to.window(driver.window_handles[1])                  \n",
    "\n",
    "    elems = driver.find_elements(by=By.XPATH, value='/html/body/app-root/app-result/div/div/div[2]/div/div/div/div/div/div[1]/table/tbody')\n",
    "    links = list(map(lambda elem: elem.find_elements(by=By.XPATH, value='.//tr[1]/td[2]/a')[0], elems))\n",
    "    \n",
    "    print(f'{len(links)} links on this page')\n",
    "    \n",
    "    for link in links:\n",
    "        y_height = 0\n",
    "        while not element_completely_viewable(driver, link):\n",
    "            driver.execute_script(f\"window.scrollTo(0, {y_height})\") \n",
    "            y_height += 100\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        link.click()\n",
    "            \n",
    "        # Switch to page with content and get URL name\n",
    "        time.sleep(1)\n",
    "        driver.switch_to.window(driver.window_handles[2])         \n",
    "        item_key = driver.current_url.split('/')[-1]         \n",
    "        item_key = item_key.replace('?', '_') # Replace ? because it would be an invalid filename \n",
    "\n",
    "        # Write out each page source as a file         \n",
    "        with open('scraped_content/' + item_key + '.txt', encoding = 'utf-8', mode = 'w+') as file:             \n",
    "            file.write(driver.page_source)    \n",
    "        docs_saved += 1\n",
    "        print(f'Saved {docs_saved} / {num_results} docs')\n",
    "\n",
    "        # Close tab         \n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if docs_saved == num_results:\n",
    "        break\n",
    "    elif docs_saved == 20:\n",
    "        next_page = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[3]/section/ul/li[1]/a/i')\n",
    "    else:        \n",
    "        next_page = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[3]/section/ul/li[3]/a/i')     \n",
    "    \n",
    "    next_page.click()\n",
    "    pages_seen += 1\n",
    "    print(f'Seen {pages_seen} / {num_pages} pages')\n",
    "\n",
    "    # Sleep momentarily because next page takes a while to load     \n",
    "    time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\AppData\\Local\\Temp\\ipykernel_17212\\3422452015.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  elem = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/table/tbody[{}]/tr[1]/td[2]/a'.format(item))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'res_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Check that all results are stored \u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mres_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(res[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like not all the results were stored!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# old codee here.\n",
    "# Nested for loop to click through all search results \n",
    "for page in range(num_pages):         \n",
    "    # This assumes that 20 search results are returned, which are 1-indexed in the xpaths     \n",
    "    for item in range (1, 21):\n",
    "        # Switch to search results page         \n",
    "        driver.switch_to.window(driver.window_handles[1])                  \n",
    "        # Get element to click on, to see each individual page with content         \n",
    "        # Last page will have fewer than 20 elements, so need to handle this exception         \n",
    "        try:             \n",
    "            elem = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/table/tbody[{}]/tr[1]/td[2]/a'.format(item))\n",
    "            elem.click()     \n",
    "            \n",
    "            # Switch to page with content and get URL name         \n",
    "            driver.switch_to.window(driver.window_handles[2])         \n",
    "            item_key = driver.current_url.split('/')[-1]         \n",
    "            item_key = item_key.replace('?', '_') # Replace ? because it would be an invalid filename \n",
    "\n",
    "            # Append result to dictionary for later processing         \n",
    "            res_dict[item_key] = driver.page_source  \n",
    "\n",
    "            # Write out each page source as a file         \n",
    "            with open('scraped_content/' + item_key + '.txt', encoding = 'utf-8', mode = 'w+') as file:             \n",
    "                file.write(driver.page_source)                  \n",
    "\n",
    "            # Close tab         \n",
    "            driver.close()              \n",
    "        \n",
    "        except:             \n",
    "            break   \n",
    "                \n",
    "        # Switch back to search results tab     \n",
    "        driver.switch_to.window(driver.window_handles[1])          \n",
    "        \n",
    "        # Click on next page once 20 results have been saved     \n",
    "        # Next page button changes after first 20 results are shown, hence the need for enclosing the element xpath in a try block     \n",
    "        try:         \n",
    "            next_page = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[3]/section/ul/li[3]/a/i')     \n",
    "        except:         \n",
    "            next_page = driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[3]/section/ul/li[1]/a/i')     \n",
    "        next_page.click()          \n",
    "        \n",
    "        # Sleep momentarily because next page takes a while to load     \n",
    "        time.sleep(2) \n",
    "\n",
    "# Check that all results are stored \n",
    "assert len(res_dict.keys()) == int(res[-1]), \"It looks like not all the results were stored!\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
